{"cells":[{"cell_type":"markdown","metadata":{"id":"mt9dL5dIir8X"},"source":["##### Copyright 2022 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ufPx7EiCiqgR"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n"]},{"cell_type":"markdown","metadata":{"id":"4StGz9ynOEL6"},"source":["# Load video data"]},{"cell_type":"markdown","metadata":{"id":"KwQtSOz0VrVX"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/video\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/load_data/video.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"F-SqCosJ6-0H"},"source":["This tutorial demonstrates how to load and preprocess [AVI](https://en.wikipedia.org/wiki/Audio_Video_Interleave) video data using the [UCF101 human action dataset](https://www.tensorflow.org/datasets/catalog/ucf101). Once you have preprocessed the data, it can be used for such tasks as video classification/recognition, captioning or clustering. The original dataset contains realistic action videos collected from YouTube with 101 categories, including playing cello, brushing teeth, and applying eye makeup. You will learn how to:\n","\n","* Load the data from a zip file.\n","\n","* Read sequences of frames out of the video files.\n","\n","* Visualize the video data.\n","\n","* Wrap the frame-generator [`tf.data.Dataset`](https://www.tensorflow.org/guide/data).\n","\n","This video loading and preprocessing tutorial is the first part in a series of TensorFlow video tutorials. Here are the other three tutorials:\n","\n","- [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification): Note that this tutorial uses a (2+1)D CNN that decomposes the spatial and temporal aspects of 3D data; if you are using volumetric data such as an MRI scan, consider using a 3D CNN instead of a (2+1)D CNN.\n","- [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet): Get familiar with the MoViNet models that are available on TF Hub.\n","- [Transfer learning for video classification with MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet): This tutorial explains how to use a pre-trained video classification model trained on a different dataset with the UCF-101 dataset."]},{"cell_type":"markdown","metadata":{"id":"PnpPjKVD68eH"},"source":["## Setup\n","\n","Begin by installing and importing some necessary libraries, including:\n","[remotezip](https://github.com/gtsystem/python-remotezip) to inspect the contents of a ZIP file, [tqdm](https://github.com/tqdm/tqdm) to use a progress bar, [OpenCV](https://opencv.org/) to process video files, and [`tensorflow_docs`](https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs) for embedding data in a Jupyter notebook."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7808,"status":"ok","timestamp":1677770416561,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"SjI3AaaO16bd","outputId":"685a51de-8fb4-4244-a591-80fc695636e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow>=2.10.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (23.0)\n","Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.11.2)\n","Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.11.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (4.5.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (23.1.21)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.51.3)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.22.4)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (3.19.6)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.15.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.6.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (0.31.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (0.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (57.4.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (15.0.6.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (0.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.15.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (3.1.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (3.3.0)\n","Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.11.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10.0) (0.38.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.25.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.2.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.16.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (3.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (6.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (4.0.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (3.2.2)\n"]}],"source":["# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n","!pip install -U \"tensorflow>=2.10.0\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19731,"status":"ok","timestamp":1677770436288,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"P5SBasQcbwQA","outputId":"60bb7b83-9e65-456f-a180-b0287b3b5aa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting remotezip\n","  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from remotezip) (2.25.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from remotezip) (0.8.10)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.22.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (2022.12.7)\n","Building wheels for collected packages: remotezip\n","  Building wheel for remotezip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7947 sha256=3be038021f0d1ad69a9c1b5030b395a47a73db6d3ea26782696ab2831b23c6c7\n","  Stored in directory: /root/.cache/pip/wheels/36/69/50/7b5a7fd4fda1cbb85c080b1c05cbbd2f88ac6a665260910b13\n","Successfully built remotezip\n","Installing collected packages: remotezip\n","Successfully installed remotezip-0.12.1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install remotezip tqdm opencv-python\n","!pip install -q git+https://github.com/tensorflow/docs"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5977,"status":"ok","timestamp":1677770442261,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"9RYQIJ9C6BVH"},"outputs":[],"source":["import tqdm\n","import random\n","import pathlib\n","import itertools\n","import collections\n","\n","import os\n","import cv2\n","import numpy as np\n","import remotezip as rz\n","\n","import tensorflow as tf\n","\n","# Some modules to display an animation using imageio.\n","import imageio\n","from IPython import display\n","from urllib import request\n","from tensorflow_docs.vis import embed"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55784,"status":"ok","timestamp":1677770498040,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"Hj5Np4dJlWpA","outputId":"688a9f2d-644d-4cb4-d86d-19bcf9a10704"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2407,"status":"ok","timestamp":1677770500440,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"fywsuV73m0fh","outputId":"bfc0f7b7-c602-4cb2-9d47-72dc6f73606a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for zipfile\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install zipfile"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677770500441,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"YBmBTOTLl-04"},"outputs":[],"source":["#@title \n","from zipfile import ZipFile\n","def list_files_per_class(zip_url):\n","  \"\"\"\n","    List the files in each class of the dataset given the zip URL.\n","\n","    Args:\n","      zip_url: URL from which the files can be unzipped. \n","\n","    Return:\n","      files: List of files in each of the classes.\n","  \"\"\"\n","  file_name=\"/content/drive/MyDrive//BiPS.zip\"\n","  files = []\n","  with ZipFile(file_name, 'r') as zip:\n","    for zip_info in zip.infolist():\n","      files.append(zip_info.filename)\n","  return files\n","\n","def get_class(fname):\n","  \"\"\"\n","    Retrieve the name of the class given a filename.\n","\n","    Args:\n","      fname: Name of the file in the UCF101 dataset.\n","\n","    Return:\n","      Class that the file belongs to.\n","  \"\"\"\n","  tokens = fname.split('/')\n","  fname=tokens[3]\n","  return fname\n","\n","def get_files_per_class(files):\n","  \"\"\"\n","    Retrieve the files that belong to each class. \n","\n","    Args:\n","      files: List of files in the dataset.\n","\n","    Return:\n","      Dictionary of class names (key) and files (values).\n","  \"\"\"\n","  files_for_class = collections.defaultdict(list)\n","  for fname in files:\n","    class_name = get_class(fname)\n","    files_for_class[class_name].append(fname)\n","  return files_for_class\n","\n","def download_from_zip(zip_url, to_dir, file_names):\n","  \"\"\"\n","    Download the contents of the zip file from the zip URL.\n","\n","    Args:\n","      zip_url: Zip URL containing data.\n","      to_dir: Directory to download data to.\n","      file_names: Names of files to download.\n","  \"\"\"\n","  zip= ZipFile(\"/content/drive/MyDrive/BiPS.zip\")\n","  for fn in tqdm.tqdm(file_names):\n","      class_name = get_class(fn)\n","      zip.extract(fn, str(to_dir / class_name))\n","      unzipped_file = to_dir / class_name\n","\n","def split_class_lists(files_for_class):\n","  \"\"\"\n","    Returns the list of files belonging to a subset of data as well as the remainder of\n","    files that need to be downloaded.\n","\n","    Args:\n","      files_for_class: Files belonging to a particular class of data.\n","      count: Number of files to download.\n","\n","    Return:\n","      split_files: Files belonging to the subset of data.\n","      remainder: Dictionary of the remainder of files that need to be downloaded.\n","  \"\"\"\n","  split_files = []\n","  remainder = {}\n","  for cls in files_for_class:\n","    split_files.extend(files_for_class[cls])\n","    remainder[cls] = files_for_class[cls]\n","  return split_files, remainder\n","\n","def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n","  \"\"\"\n","    Download a subset of the UFC101 dataset and split them into various parts, such as\n","    training, validation, and test. \n","\n","    Args:\n","      zip_url: Zip URL containing data.\n","      num_classes: Number of labels.\n","      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n","              (value is number of files per split).\n","      download_dir: Directory to download data to.\n","\n","    Return:\n","      dir: Posix path of the resulting directories containing the splits of data.\n","  \"\"\"\n","  files = list_files_per_class(zip_url)\n","  for f in files:\n","    tokens = f.split('/')\n","    if len(tokens)<5:\n","      files.remove(f)\n","    elif (\"\" in tokens):\n","      files.remove(f) # Remove that item from the list if it does not have a filenamee\n","\n","  files_for_class = get_files_per_class(files)\n","\n","  classes = list(files_for_class.keys())[:num_classes]\n","\n","  for cls in classes:\n","    new_files_for_class = files_for_class[cls]\n","    random.shuffle(new_files_for_class)\n","    files_for_class[cls] = new_files_for_class\n","\n","  # Only use the number of classes you want in the dictionary\n","  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n","\n","  dirs = {}\n","  for split_name, split_count in splits.items():\n","    print(split_name, \":\")\n","    split_dir = download_dir / split_name\n","    split_files, files_for_class = split_class_lists(files_for_class)\n","    #download_from_zip(zip_url, split_dir, split_files)\n","    dirs[split_name] = split_dir\n","\n","  return dirs, split_files,split_dir\n","\n","class FrameGenerator:\n","  def __init__(self, path, n_frames, training = False):\n","    \"\"\" Returns a set of frames with their associated label. \n","\n","      Args:\n","        path: Video file paths.\n","        n_frames: Number of frames. \n","        training: Boolean to determine if training dataset is being created.\n","    \"\"\"\n","    self.path = path\n","    self.n_frames = n_frames\n","    self.training = training\n","    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n","    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n","\n","  def get_files_and_class_names(self):\n","    video_paths = list(self.path.glob('*/*.mp4'))\n","    classes = [p.parent.name for p in video_paths] \n","    return video_paths, classes\n","\n","  def __call__(self):\n","    video_paths, classes = self.get_files_and_class_names()\n","\n","    pairs = list(zip(video_paths, classes))\n","\n","    if self.training:\n","      random.shuffle(pairs)\n","\n","    for path, name in pairs:\n","      video_frames = frames_from_video_file(path, self.n_frames) \n","      label = self.class_ids_for_name[name] # Encode labels\n","      yield video_frames, label"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39027,"status":"ok","timestamp":1677770539461,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"qS7c8TO_mEIw","outputId":"4c948a14-733e-441f-e947-45984c704abc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3414"]},"metadata":{},"execution_count":7}],"source":["subset_paths={'video_train': pathlib.Path('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/train'), 'video_val': pathlib.Path('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/val'), 'video_test': pathlib.Path('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/test')}\n","download_dir=pathlib.Path('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/')\n","video_count_train = len(list(download_dir.glob('train/*/*.mp4')))\n","video_count_val = len(list(download_dir.glob('val/*/*.mp4')))\n","video_count_test = len(list(download_dir.glob('test/*/*.mp4')))\n","video_total = video_count_train + video_count_val + video_count_test\n","video_count_train"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1677770539462,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"uchOyxUOnmgR","outputId":"f783eb87-a5cf-42dd-8d61-8716a0ba8145"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'video_train': PosixPath('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/train'),\n"," 'video_val': PosixPath('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/val'),\n"," 'video_test': PosixPath('/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/Folder/test')}"]},"metadata":{},"execution_count":8}],"source":["subset_paths"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1677770539462,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"0G39sflanCww","outputId":"c3ca096f-cd69-4169-e0f1-604a945c5285"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["427"]},"metadata":{},"execution_count":9}],"source":["video_count_val"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677770539462,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"YW81jTvQnE6T","outputId":"8f77e56a-c118-4f1c-d2e0-6b4c95355bf3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["427"]},"metadata":{},"execution_count":10}],"source":["video_count_test"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677770539463,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"mNIhDK2qoTFI","outputId":"5f0995f3-8971-4113-da70-92be5784cd42"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4268"]},"metadata":{},"execution_count":11}],"source":["video_total"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":689,"status":"ok","timestamp":1677770540146,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"uYe4RM8Vml2O"},"outputs":[],"source":["import pandas as pd\n","label_data = pd.read_excel(\"/content/drive/MyDrive/Đồ án DS201 - Deep Learning/[1] Data/Data train model 2 label/video_train.xlsx\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1677770540146,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"BfyLDX9dnFtz","outputId":"d8c0acc1-1714-44f0-de30-469008c54780"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0          Normal        Abnormal\n","0            0.0  video_5151.mp4  video_0881.mp4\n","1            1.0  video_3858.mp4  video_4275.mp4\n","2            2.0  video_5313.mp4  video_1068.mp4\n","3            3.0  video_2660.mp4  video_1437.mp4\n","4            4.0  video_4848.mp4  video_0398.mp4\n","...          ...             ...             ...\n","1755      1755.0  video_2921.mp4             NaN\n","1756      1756.0  video_5303.mp4             NaN\n","1757      1757.0  video_5108.mp4             NaN\n","1758      1758.0  video_0915.mp4             NaN\n","1759      1759.0  video_0230.mp4             NaN\n","\n","[1760 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-a32e4cbd-a2ac-4484-96a8-8d6fce29e331\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Normal</th>\n","      <th>Abnormal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>video_5151.mp4</td>\n","      <td>video_0881.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>video_3858.mp4</td>\n","      <td>video_4275.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>video_5313.mp4</td>\n","      <td>video_1068.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>video_2660.mp4</td>\n","      <td>video_1437.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>video_4848.mp4</td>\n","      <td>video_0398.mp4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1755</th>\n","      <td>1755.0</td>\n","      <td>video_2921.mp4</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1756</th>\n","      <td>1756.0</td>\n","      <td>video_5303.mp4</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1757</th>\n","      <td>1757.0</td>\n","      <td>video_5108.mp4</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1758</th>\n","      <td>1758.0</td>\n","      <td>video_0915.mp4</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1759</th>\n","      <td>1759.0</td>\n","      <td>video_0230.mp4</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1760 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a32e4cbd-a2ac-4484-96a8-8d6fce29e331')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a32e4cbd-a2ac-4484-96a8-8d6fce29e331 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a32e4cbd-a2ac-4484-96a8-8d6fce29e331');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}],"source":["label_data"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677770540147,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"ezYLzaUfoV01"},"outputs":[],"source":["label_data.drop(['Unnamed: 0'], axis=1, inplace=True)\n","label_data = pd.DataFrame({\"labels\": label_data.columns})"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677770540147,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"M-Yg17Ghodfa","outputId":"0cb4371d-d8ce-45bf-ac0d-2e47903df877"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     labels\n","0    Normal\n","1  Abnormal"],"text/html":["\n","  <div id=\"df-e3e9ac68-1a7d-4538-8e3e-606aad40360e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Abnormal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3e9ac68-1a7d-4538-8e3e-606aad40360e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e3e9ac68-1a7d-4538-8e3e-606aad40360e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e3e9ac68-1a7d-4538-8e3e-606aad40360e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}],"source":["label_data"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3992,"status":"ok","timestamp":1677770544126,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"WE3ipOUimKLn"},"outputs":[],"source":["batch_size = 8\n","num_frames = 8\n","\n","output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n","                    tf.TensorSpec(shape = (), dtype = tf.int16))\n","\n","train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['video_train'], num_frames, training = True),\n","                                          output_signature = output_signature)\n","train_ds = train_ds.batch(batch_size)\n","val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['video_val'], num_frames),\n","                                         output_signature = output_signature)\n","val_ds = val_ds.batch(batch_size)\n","test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['video_test'], num_frames),\n","                                         output_signature = output_signature)\n","test_ds = test_ds.batch(batch_size)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677770544127,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"7Wz8o_kZmM9I"},"outputs":[],"source":["fg = FrameGenerator(subset_paths['video_train'], num_frames, training = True)\n","label_names = list(fg.class_ids_for_name.keys())"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677770544127,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"PC9Hkp5xoxvq","outputId":"344a43f9-f2bb-4895-b9ee-18329c667cbb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Abnormal', 'Normal']"]},"metadata":{},"execution_count":18}],"source":["label_names"]},{"cell_type":"markdown","metadata":{"id":"MBMRm9Ub3Zrk"},"source":["After downloading the data, you should now have a copy of a subset of the UCF101 dataset. Run the following code to print the total number of videos you have amongst all your subsets of data."]},{"cell_type":"markdown","metadata":{"id":"JmJG1SlXiOX8"},"source":["You can also preview the directory of data files now."]},{"cell_type":"markdown","metadata":{"id":"U4uslY4dScyu"},"source":["## Create frames from each video file"]},{"cell_type":"markdown","metadata":{"id":"D1vvyT0F7JAZ"},"source":["The `frames_from_video_file` function splits the videos into frames, reads a randomly chosen span of `n_frames` out of a video file, and returns them as a NumPy `array`.\n","To reduce memory and computation overhead, choose a **small** number of frames. In addition, pick the **same** number of frames from each video, which makes it easier to work on batches of data.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677770544127,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"vNBCiV3bMzpD"},"outputs":[],"source":["def format_frames(frame, output_size):\n","  \"\"\"\n","    Pad and resize an image from a video.\n","    \n","    Args:\n","      frame: Image that needs to resized and padded. \n","      output_size: Pixel size of the output frame image.\n","\n","    Return:\n","      Formatted frame with padding of specified output size.\n","  \"\"\"\n","  frame = tf.image.convert_image_dtype(frame, tf.float32)\n","  frame = tf.image.resize_with_pad(frame, *output_size)\n","  return frame"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677770544128,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"9ujLDC9G7JyE"},"outputs":[],"source":["def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n","  \"\"\"\n","    Creates frames from each video file present for each category.\n","\n","    Args:\n","      video_path: File path to the video.\n","      n_frames: Number of frames to be created per video file.\n","      output_size: Pixel size of the output frame image.\n","\n","    Return:\n","      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n","  \"\"\"\n","  # Read each video frame by frame\n","  result = []\n","  src = cv2.VideoCapture(str(video_path))  \n","\n","  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n","\n","  need_length = 1 + (n_frames - 1) * frame_step\n","\n","  if need_length > video_length:\n","    start = 0\n","  else:\n","    max_start = video_length - need_length\n","    start = random.randint(0, max_start + 1)\n","\n","  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n","  # ret is a boolean indicating whether read was successful, frame is the image itself\n","  ret, frame = src.read()\n","  result.append(format_frames(frame, output_size))\n","\n","  for _ in range(n_frames - 1):\n","    for _ in range(frame_step):\n","      ret, frame = src.read()\n","    if ret:\n","      frame = format_frames(frame, output_size)\n","      result.append(frame)\n","    else:\n","      result.append(np.zeros_like(result[0]))\n","  src.release()\n","  result = np.array(result)[..., [2, 1, 0]]\n","\n","  return result"]},{"cell_type":"markdown","metadata":{"id":"1ENtlwhxwyTe"},"source":["## Visualize video data\n","\n","The `frames_from_video_file` function that returns a set of frames as a NumPy array. Try using this function on a new video from [Wikimedia](https://commons.wikimedia.org/wiki/Category:Videos_of_sports){:.external} by Patrick Gillett:"]},{"cell_type":"markdown","metadata":{"id":"3dktTnDVG7xf"},"source":["In addition to examining this video, you can also display the UCF-101 data. To do this, run the following code:"]},{"cell_type":"markdown","metadata":{"id":"NlvuC5_E7XrF"},"source":["Next, define the `FrameGenerator` class in order to create an iterable object that can feed data into the TensorFlow data pipeline. The generator (`__call__`) function yields the frame array produced by `frames_from_video_file` and a one-hot encoded vector of the label associated with the set of frames."]},{"cell_type":"markdown","metadata":{"id":"xsvhPIkpzx-r"},"source":["Test out the `FrameGenerator` object before wrapping it as a TensorFlow Dataset object. Moreover, for the training dataset, ensure you enable training mode so that the data will be shuffled."]},{"cell_type":"markdown","metadata":{"id":"E7MRRFSks7l1"},"source":["Finally, create a TensorFlow data input pipeline. This pipeline that you create from the generator object allows you to feed in data to your deep learning model. In this video pipeline, each element is a single set of frames and its associated label. "]},{"cell_type":"markdown","metadata":{"id":"9oF_8m8IZvcY"},"source":["Check to see that the labels are shuffled. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":906},"executionInfo":{"elapsed":115617,"status":"error","timestamp":1677665530418,"user":{"displayName":"Tâm Nguyễn Hữu Minh","userId":"12698700356957555938"},"user_tz":-420},"id":"3XYVmsgiZsJD","outputId":"a00afc45-1aa8-428e-8c77-98fc5ceecbfc"},"outputs":[{"ename":"InvalidArgumentError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-a1709413c279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    771\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7214\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7215\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 241, in __call__\n    def __call__(self, token, device, args):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1039, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-6-91d46e8e96d6>\", line 159, in __call__\n    video_frames = frames_from_video_file(path, self.n_frames)\n\n  File \"<ipython-input-25-5e19e2897998>\", line 30, in frames_from_video_file\n    result.append(format_frames(frame, output_size))\n\n  File \"<ipython-input-24-39672ccafba1>\", line 12, in format_frames\n    frame = tf.image.convert_image_dtype(frame, tf.float32)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"]}],"source":["# for frames, labels in train_ds.take(10):\n","#   print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6qXc-6i7eyK"},"outputs":[],"source":["# # Print the shapes of the data\n","# train_frames, train_labels = next(iter(train_ds))\n","# print(f'Shape of training set of frames: {train_frames.shape}')\n","# print(f'Shape of training labels: {train_labels.shape}')\n","\n","# val_frames, val_labels = next(iter(val_ds))\n","# print(f'Shape of validation set of frames: {val_frames.shape}')\n","# print(f'Shape of validation labels: {val_labels.shape}')"]},{"cell_type":"markdown","metadata":{"id":"hqjXn1FgsMqZ"},"source":["## Next steps\n","\n","Now that you have created a TensorFlow `Dataset` of video frames with their labels, you can use it with a deep learning model. The following classification model that uses a pre-trained [EfficientNet](https://arxiv.org/abs/1905.11946){:.external} trains to high accuracy in a few minutes:"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677770544128,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"},"user_tz":-420},"id":"ti6KkewqVejG"},"outputs":[],"source":["checkpoint_dir = '/content/drive/MyDrive/DS210_Model/2_labels'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBr6GRjbWNZT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzqgPBUuForj","executionInfo":{"status":"ok","timestamp":1677779664247,"user_tz":-420,"elapsed":9120124,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"}},"outputId":"62641c94-485b-4a4d-9852-9bea920ccc97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","16705208/16705208 [==============================] - 2s 0us/step\n","Epoch 3/3\n","1281/1281 [==============================] - 9046s 7s/step - loss: 0.5354 - accuracy: 0.7194 - val_loss: 0.5494 - val_accuracy: 0.7237\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f33202046d0>"]},"metadata":{},"execution_count":22}],"source":["net = tf.keras.applications.EfficientNetB0(include_top = False)\n","net.trainable = False\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Rescaling(scale=255),\n","    tf.keras.layers.TimeDistributed(net),\n","    tf.keras.layers.Dense(2),\n","    tf.keras.layers.GlobalAveragePooling3D()\n","])\n","\n","model.compile(optimizer = 'adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n","              metrics=['accuracy'])\n","\n","model.fit(train_ds, \n","          epochs = 3,\n","          validation_data = val_ds,\n","          callbacks = tf.keras.callbacks.BackupAndRestore(checkpoint_dir, save_freq='epoch', delete_checkpoint=False, save_before_preemption=False))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"elmzYAmRqpH8","executionInfo":{"status":"ok","timestamp":1677779665319,"user_tz":-420,"elapsed":3,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"}}},"outputs":[],"source":["def get_actual_predicted_labels(dataset): \n","  \"\"\"\n","    Create a list of actual ground truth values and the predictions from the model.\n","\n","    Args:\n","      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n","\n","    Return:\n","      Ground truth and predicted values for a particular dataset.\n","  \"\"\"\n","  actual = [labels for _, labels in dataset.unbatch()]\n","  predicted = model.predict(dataset)\n","\n","  actual = tf.stack(actual, axis=0)\n","  predicted = tf.concat(predicted, axis=0)\n","  predicted = tf.argmax(predicted, axis=1)\n","\n","  return actual, predicted"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"9k6I-sf8foC8","executionInfo":{"status":"ok","timestamp":1677779665319,"user_tz":-420,"elapsed":3,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"}}},"outputs":[],"source":["fg = FrameGenerator(subset_paths['video_train'], num_frames, training=True)\n","labels = list(fg.class_ids_for_name.keys())"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5mwZEDjgCRj","outputId":"09858dc6-1750-4953-f061-b190ad46c4c3","executionInfo":{"status":"ok","timestamp":1677780753031,"user_tz":-420,"elapsed":1087715,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["54/54 [==============================] - 377s 7s/step\n"]}],"source":["actual, predicted = get_actual_predicted_labels(test_ds)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKdDRIT5gC5c","outputId":"7a092ce9-730b-4e8f-ce60-535943c67ea1","executionInfo":{"status":"ok","timestamp":1677780753031,"user_tz":-420,"elapsed":14,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6979    0.7923    0.7421       207\n","           1     0.7760    0.6773    0.7233       220\n","\n","    accuracy                         0.7330       427\n","   macro avg     0.7370    0.7348    0.7327       427\n","weighted avg     0.7381    0.7330    0.7324       427\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","print(classification_report(actual, predicted,digits=4))"]},{"cell_type":"code","source":["accuracy"],"metadata":{"id":"Ov3r6maljHfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gma049hkfbC8"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","def plot_confusion_matrix(actual, predicted, labels, ds_type):\n","  cm = tf.math.confusion_matrix(actual, predicted)\n","  ax = sns.heatmap(cm, annot=True, fmt='g')\n","  sns.set(rc={'figure.figsize':(12, 12)})\n","  sns.set(font_scale=1.4)\n","  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n","  ax.set_xlabel('Predicted Action')\n","  ax.set_ylabel('Actual Action')\n","  plt.xticks(rotation=90)\n","  plt.yticks(rotation=0)\n","  ax.xaxis.set_ticklabels(labels)\n","  ax.yaxis.set_ticklabels(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lv68ELVCfdhG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uyy84Vr7aB6K","outputId":"9d9de633-f04e-449f-d752-7c8da189b79a","executionInfo":{"status":"ok","timestamp":1677781716010,"user_tz":-420,"elapsed":382556,"user":{"displayName":"Tammm Nguyen","userId":"04452848346074037133"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["54/54 [==============================] - 382s 7s/step - loss: 0.5447 - accuracy: 0.7307\n"]},{"output_type":"execute_result","data":{"text/plain":["{'loss': 0.5446543097496033, 'accuracy': 0.7306791543960571}"]},"metadata":{},"execution_count":27}],"source":["model.evaluate(test_ds, return_dict=True)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}